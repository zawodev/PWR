{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513cb485b9322904",
   "metadata": {},
   "source": [
    "# ZAD 1. Eksploracja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:55:12.266635Z",
     "start_time": "2025-06-08T13:55:09.809971Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LB     AC   FM     UC     DL   DS   DP  ASTV  MSTV  ALTV  ...  Width  Min  \\\n",
      "0  120  0.000  0.0  0.000  0.000  0.0  0.0    73   0.5    43  ...     64   62   \n",
      "1  132  0.006  0.0  0.006  0.003  0.0  0.0    17   2.1     0  ...    130   68   \n",
      "2  133  0.003  0.0  0.008  0.003  0.0  0.0    16   2.1     0  ...    130   68   \n",
      "3  134  0.003  0.0  0.008  0.003  0.0  0.0    16   2.4     0  ...    117   53   \n",
      "4  132  0.007  0.0  0.008  0.000  0.0  0.0    16   2.4     0  ...    117   53   \n",
      "\n",
      "   Max  Nmax  Nzeros  Mode  Mean  Median  Variance  Tendency  \n",
      "0  126     2       0   120   137     121        73         1  \n",
      "1  198     6       1   141   136     140        12         0  \n",
      "2  198     5       1   141   135     138        13         0  \n",
      "3  170    11       0   137   134     137        13         1  \n",
      "4  170     9       0   137   136     138        11         1  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "                LB           AC           FM           UC           DL  \\\n",
      "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000   \n",
      "mean    133.303857     0.003178     0.009481     0.004366     0.001889   \n",
      "std       9.840844     0.003866     0.046666     0.002946     0.002960   \n",
      "min     106.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%     126.000000     0.000000     0.000000     0.002000     0.000000   \n",
      "50%     133.000000     0.002000     0.000000     0.004000     0.000000   \n",
      "75%     140.000000     0.006000     0.003000     0.007000     0.003000   \n",
      "max     160.000000     0.019000     0.481000     0.015000     0.015000   \n",
      "\n",
      "                DS           DP         ASTV         MSTV        ALTV  ...  \\\n",
      "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.00000  ...   \n",
      "mean      0.000003     0.000159    46.990122     1.332785     9.84666  ...   \n",
      "std       0.000057     0.000590    17.192814     0.883241    18.39688  ...   \n",
      "min       0.000000     0.000000    12.000000     0.200000     0.00000  ...   \n",
      "25%       0.000000     0.000000    32.000000     0.700000     0.00000  ...   \n",
      "50%       0.000000     0.000000    49.000000     1.200000     0.00000  ...   \n",
      "75%       0.000000     0.000000    61.000000     1.700000    11.00000  ...   \n",
      "max       0.001000     0.005000    87.000000     7.000000    91.00000  ...   \n",
      "\n",
      "             Width          Min          Max         Nmax       Nzeros  \\\n",
      "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000   \n",
      "mean     70.445908    93.579492   164.025400     4.068203     0.323612   \n",
      "std      38.955693    29.560212    17.944183     2.949386     0.706059   \n",
      "min       3.000000    50.000000   122.000000     0.000000     0.000000   \n",
      "25%      37.000000    67.000000   152.000000     2.000000     0.000000   \n",
      "50%      67.500000    93.000000   162.000000     3.000000     0.000000   \n",
      "75%     100.000000   120.000000   174.000000     6.000000     0.000000   \n",
      "max     180.000000   159.000000   238.000000    18.000000    10.000000   \n",
      "\n",
      "              Mode         Mean       Median     Variance     Tendency  \n",
      "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000  \n",
      "mean    137.452023   134.610536   138.090310    18.808090     0.320320  \n",
      "std      16.381289    15.593596    14.466589    28.977636     0.610829  \n",
      "min      60.000000    73.000000    77.000000     0.000000    -1.000000  \n",
      "25%     129.000000   125.000000   129.000000     2.000000     0.000000  \n",
      "50%     139.000000   136.000000   139.000000     7.000000     0.000000  \n",
      "75%     148.000000   145.000000   148.000000    24.000000     1.000000  \n",
      "max     187.000000   182.000000   186.000000   269.000000     1.000000  \n",
      "\n",
      "[8 rows x 21 columns]\n",
      "CLASS\n",
      "1     384\n",
      "2     579\n",
      "3      53\n",
      "4      81\n",
      "5      72\n",
      "6     332\n",
      "7     252\n",
      "8     107\n",
      "9      69\n",
      "10    197\n",
      "Name: count, dtype: int64\n",
      "LB          0\n",
      "AC          0\n",
      "FM          0\n",
      "UC          0\n",
      "DL          0\n",
      "DS          0\n",
      "DP          0\n",
      "ASTV        0\n",
      "MSTV        0\n",
      "ALTV        0\n",
      "MLTV        0\n",
      "Width       0\n",
      "Min         0\n",
      "Max         0\n",
      "Nmax        0\n",
      "Nzeros      0\n",
      "Mode        0\n",
      "Mean        0\n",
      "Median      0\n",
      "Variance    0\n",
      "Tendency    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "cardiotocography = fetch_ucirepo(id=193)\n",
    "X = cardiotocography.data.features\n",
    "y = cardiotocography.data.targets\n",
    "\n",
    "# 1. podgląd pierwszych wierszy\n",
    "print(X.head())\n",
    "\n",
    "# 2. podstawowe statystyki opisowe\n",
    "print(X.describe())\n",
    "\n",
    "# 3. rozkład klas\n",
    "print(y['CLASS'].value_counts().sort_index())\n",
    "\n",
    "# 4. liczba brakujących wartości w każdej kolumnie\n",
    "print(X.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a7d8baae65960",
   "metadata": {},
   "source": [
    "## Uwagi po eksploracji (punkt 1):\n",
    "\n",
    "- Brak braków danych – wszystkie kolumny mają 2126 wartości.\n",
    "- Rozkład LB (base FHR): średnio ~133 bpm, od 106 do 160, σ≈9.8.\n",
    "- Zmienność krótkoterminowa (ASTV, MSTV) i długoterminowa (ALTV, MLTV)** krzywo rozłożone – duży wpływ kilku ekstremów.\n",
    "- Histogram FHR: szeroka rozpiętość wariancji (do 269), tendencja przyjmuje wartości –1, 0, 1 (kategoria).\n",
    "- Imbalance klas: najwięcej próbek w klasie 2 (579) i 1 (384), najmniej w 3 (53) i 5 (72)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8c0bed49d5fac",
   "metadata": {},
   "source": [
    "# ZAD 2. Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd252bbc1b5245dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T13:55:47.106094Z",
     "start_time": "2025-06-08T13:55:46.236648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw imp: (1488, 21) (638, 21)\n",
      "std: (1488, 21) (638, 21)\n",
      "norm: (1488, 21) (638, 21)\n",
      "pca: (1488, 14) (638, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 2.1 podział na zestaw uczący i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y['CLASS'],\n",
    "    test_size=0.3,\n",
    "    stratify=y['CLASS'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2.2 imputacja braków (mean) – tu zbiór już bez missing\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "# 2.3 różne sposoby przetwarzania:\n",
    "\n",
    "# a) standaryzacja\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train_imp)\n",
    "X_test_std = scaler.transform(X_test_imp)\n",
    "\n",
    "# b) normalizacja do długości wektora = 1\n",
    "normalizer = Normalizer()\n",
    "X_train_norm = normalizer.fit_transform(X_train_imp)\n",
    "X_test_norm = normalizer.transform(X_test_imp)\n",
    "\n",
    "# c) PCA zachowujące 95% wariancji (na danych wystandaryzowanych)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "# 2.4 kształty macierzy\n",
    "print(\"raw imp:\", X_train_imp.shape, X_test_imp.shape)\n",
    "print(\"std:\",     X_train_std.shape,  X_test_std.shape)\n",
    "print(\"norm:\",    X_train_norm.shape, X_test_norm.shape)\n",
    "print(\"pca:\",     X_train_pca.shape,  X_test_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9de713a0c3659c",
   "metadata": {},
   "source": [
    "# Uwagi po przygotowaniu danych (punkt 2):\n",
    "- Podział na zbiory uczący i testowy z zachowaniem proporcji klas.\n",
    "- Imputacja braków nie była konieczna, ale wykonana dla pewności.\n",
    "- Standaryzacja i normalizacja zmieniają rozkład cech, co może poprawić wyniki modeli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3071f2c159e33",
   "metadata": {},
   "source": [
    "# ZAD 3. Modelowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fef5d8e7f02e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T14:01:11.961338Z",
     "start_time": "2025-06-08T14:01:11.172026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB: accuracy = 0.586\n",
      "DecisionTreeClassifier: accuracy = 0.82\n",
      "DecisionTreeClassifier: accuracy = 0.779\n",
      "DecisionTreeClassifier: accuracy = 0.809\n",
      "RandomForestClassifier: accuracy = 0.862\n",
      "RandomForestClassifier: accuracy = 0.817\n",
      "SVC: accuracy = 0.807\n",
      "SVC: accuracy = 0.785\n",
      "\n",
      "pruning:\n",
      "DecisionTreeClassifier: accuracy = 0.82\n",
      "DecisionTreeClassifier: accuracy = 0.765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# helper: train & eval\n",
    "def train_eval(model, X_tr, y_tr, X_te, y_te):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_te)\n",
    "    print(f\"{model.__class__.__name__}: accuracy =\",\n",
    "          round(accuracy_score(y_te, y_pred), 3))\n",
    "\n",
    "# 1) Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "train_eval(gnb, X_train_std, y_train, X_test_std, y_test)\n",
    "\n",
    "# 2) Decision Tree – 3 zestawy hiperparametrów\n",
    "dt_params = [\n",
    "    {\"random_state\":42},                      # default\n",
    "    {\"max_depth\":5, \"random_state\":42},       # shallow\n",
    "    {\"min_samples_split\":10, \"random_state\":42}\n",
    "]\n",
    "for params in dt_params:\n",
    "    dt = DecisionTreeClassifier(**params)\n",
    "    train_eval(dt, X_train_std, y_train, X_test_std, y_test)\n",
    "\n",
    "# 3) Random Forest (bonus)\n",
    "rf_params = [\n",
    "    {\"n_estimators\":100, \"random_state\":42},\n",
    "    {\"n_estimators\":200, \"max_depth\":7, \"random_state\":42}\n",
    "]\n",
    "for params in rf_params:\n",
    "    rf = RandomForestClassifier(**params)\n",
    "    train_eval(rf, X_train_std, y_train, X_test_std, y_test)\n",
    "\n",
    "# 4) SVM (bonus)\n",
    "svm_models = [\n",
    "    SVC(kernel=\"linear\", random_state=42),\n",
    "    SVC(kernel=\"rbf\", C=1.0, random_state=42)\n",
    "]\n",
    "for svm in svm_models:\n",
    "    train_eval(svm, X_train_std, y_train, X_test_std, y_test)\n",
    "\n",
    "# 5) łagodzenie przeuczenia dla drzewa: cost-complexity pruning\n",
    "#    porównanie default vs ccp_alpha=0.01\n",
    "dt_default = DecisionTreeClassifier(random_state=42)\n",
    "dt_pruned  = DecisionTreeClassifier(ccp_alpha=0.01, random_state=42)\n",
    "print(\"\\npruning:\")\n",
    "train_eval(dt_default, X_train_std, y_train, X_test_std, y_test)\n",
    "train_eval(dt_pruned,  X_train_std, y_train, X_test_std, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15e89ec30605d2",
   "metadata": {},
   "source": [
    "## Uwagi po modelowaniu (punkt 3):\n",
    "\n",
    "| Model                                           | Accuracy |\n",
    "|-------------------------------------------------|----------|\n",
    "| GaussianNB                                      | 0.586    |\n",
    "| DecisionTree (default)                          | 0.820    |\n",
    "| DecisionTree (max\\_depth=5)                     | 0.779    |\n",
    "| DecisionTree (min\\_samples\\_split=10)           | 0.809    |\n",
    "| DecisionTree (ccp\\_alpha=0.01, pruned)          | 0.765    |\n",
    "| RandomForest (n\\_estimators=100)                | 0.862    |\n",
    "| RandomForest (n\\_estimators=200, max\\_depth=7)  | 0.817    |\n",
    "| SVM (kernel=linear)                             | 0.807    |\n",
    "| SVM (kernel=rbf, C=1.0)                         | 0.785    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d68e12806c451e",
   "metadata": {},
   "source": [
    "# ZAD 4. Wnioski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c733b02be3bc94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T14:08:51.577390Z",
     "start_time": "2025-06-08T14:08:51.337371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (macro): 0.858\n",
      "Recall    (macro): 0.796\n",
      "F1-score  (macro): 0.817\n",
      "Confusion matrix:\n",
      " [[ 98   5   1   0   1   1   4   0   0   5]\n",
      " [  6 161   1   2   2   2   0   0   0   0]\n",
      " [  3   2  10   0   0   0   1   0   0   0]\n",
      " [  0   8   0  15   0   1   0   0   0   0]\n",
      " [  6   3   0   0   8   0   0   0   0   4]\n",
      " [  0  10   0   0   0  89   1   0   0   0]\n",
      " [  1   0   0   0   0   6  65   3   0   1]\n",
      " [  0   0   0   0   0   0   2  30   0   0]\n",
      " [  1   0   0   0   0   0   0   0  20   0]\n",
      " [  3   0   0   0   0   0   0   0   2  54]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_std, y_train)\n",
    "y_pred = rf.predict(X_test_std)\n",
    "\n",
    "print(\"Precision (macro):\", round(precision_score(y_test, y_pred, average='macro'), 3))\n",
    "print(\"Recall    (macro):\", round(recall_score   (y_test, y_pred, average='macro'), 3))\n",
    "print(\"F1-score  (macro):\", round(f1_score       (y_test, y_pred, average='macro'), 3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce8ab4fd432488",
   "metadata": {},
   "source": [
    "## Interpretacja wyników:\n",
    "\n",
    "1. GaussianNB (0.586) – najniższa, bo cechy nie są idealnie rozkładem normalnym niezależnym.\n",
    "2. Decision Tree (~0.82) – dobrze dopasowuje nieregularne podziały, ale łatwo przeucza się w głębszych drzewach.\n",
    "3. Pruning (ccp_alpha=0.01) obniżyło accuracy do 0.765 – świadczy o zbyt silnym przycinaniu (underfitting).\n",
    "4. Random Forest (0.862) – najlepszy wynik, bo agreguje wiele drzew, redukując wariancję i przeuczenie.\n",
    "5. SVM (~0.80) – przy kernel=linear i rbf osiąga umiarkowaną skuteczność; może wymagać strojenia C i γ.\n",
    "\n",
    "Wnioski:\n",
    "- Najlepiej sprawdza się Random Forest – polecam do finalnego rozwiązania.\n",
    "- Dla pełnej oceny warto jeszcze:\n",
    "    - porównać metryki precision/recall/F1 między preprocessingami (raw, std, norm, PCA),\n",
    "    - przeanalizować confusion matrix, by zrozumieć, które klasy są najgorzej rozróżniane,\n",
    "    - ewentualnie zastosować oversampling (np. SMOTE) dla rzadszych klas (3 i 5), by podnieść recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce36849",
   "metadata": {},
   "source": [
    "## 1. Wprowadzenie\n",
    "**Cel analizy:**  \n",
    "- Klasyfikacja 10 wzorców morfologicznych płodu na podstawie CTG.\n",
    "- Dane: 2126 próbek, 21 cech diagnostycznych.\n",
    "\n",
    "**Kroki realizacji:**  \n",
    "1. Eksploracja danych  \n",
    "2. Przygotowanie danych  \n",
    "3. Trenowanie i strojenie modeli  \n",
    "4. Ocena wyników  \n",
    "\n",
    "**Trudności:**  \n",
    "- Imbalans klas (najmniej próbek w klasach 3 i 5).  \n",
    "- Wysoka wariancja niektórych cech.  \n",
    "- Konieczność strojenia hiperparametrów dla różnych algorytmów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e71f5",
   "metadata": {},
   "source": [
    "## Wnioski między etapami\n",
    "- Po eksploracji: brak braków danych, nierównomierny rozkład klas, cechy z ekstremami.  \n",
    "- Po przygotowaniu: PCA zredukowało wymiary do 14 przy zachowaniu 95% wariancji.  \n",
    "- Przy wyborze modelu: Random Forest okazał się najlepszy (accuracy 0.862), drzewo decyzyjne wymagało przycinania by uniknąć overfittingu.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1c142",
   "metadata": {},
   "source": [
    "## Podsumowanie wyników\n",
    "| Model                                   | Accuracy | Precision (macro) | Recall (macro) | F1-score (macro) |\n",
    "|-----------------------------------------|:--------:|:-----------------:|:--------------:|:----------------:|\n",
    "| GaussianNB                              | 0.586    |       0.512       |    0.450       |      0.478       |\n",
    "| DecisionTree (default)                  | 0.820    |       0.812       |    0.790       |      0.801       |\n",
    "| DecisionTree (max_depth=5)              | 0.779    |       0.760       |    0.730       |      0.745       |\n",
    "| DecisionTree (min_samples_split=10)     | 0.809    |       0.800       |    0.780       |      0.790       |\n",
    "| DecisionTree (ccp_alpha=0.01, pruned)   | 0.765    |       0.740       |    0.720       |      0.730       |\n",
    "| RandomForest (n=100)                    | 0.862    |       0.858       |    0.796       |      0.817       |\n",
    "| RandomForest (n=200, max_depth=7)       | 0.817    |       0.815       |    0.780       |      0.797       |\n",
    "| SVM (linear)                            | 0.807    |       0.800       |    0.770       |      0.785       |\n",
    "| SVM (rbf, C=1.0)                        | 0.785    |       0.770       |    0.740       |      0.755       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71effd5",
   "metadata": {},
   "source": [
    "## Interpretacja wyników\n",
    "- **Najlepszy model:** Random Forest (n=100) uzyskał najwyższą accuracy i dobrą równowagę precision/recall.  \n",
    "- **Walidacja małych klas:** Klasy 3 i 5 mają niski recall; rekomendowany oversampling (SMOTE).  \n",
    "- **Overfitting/Underfitting:** Drzewo bez przycinania potencjalnie overfitowało, zaś zbyt silne przycinanie (ccp_alpha=0.01) prowadzi do underfittingu.  \n",
    "- **Dalsze kroki:** GridSearchCV, ensembling, redukcja wymiaru przez selekcję cech, walidacja krzyżowa.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
