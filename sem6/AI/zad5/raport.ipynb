{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sztuczna inteligencja i inżynieria wiedzy \n",
    "### Lista 5 - Weryfikacja twarzy CelebA\n",
    "##### Aleksander Stepaniuk 272644\n",
    "\n",
    "\n",
    "**Spis treści**  \n",
    "1. Wstęp i konfiguracja  \n",
    "2. Zadanie 1: Wpływ rozmiaru zbioru treningowego  \n",
    "3. Zadanie 2: Wpływ tempa uczenia (learning rate)\n",
    "4. Zadanie 3: Wpływ liczby epok  \n",
    "5. Zadanie 4: Analiza i dobór parametrów (scheduler + early stopping)\n",
    "6. Zadanie 5: Odporność na zaburzenia (augmentacje)\n",
    "7. Podsumowanie i wnioski  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Wstęp i konfiguracja\n",
    "\n",
    "W tej sekcji zaimportujemy wszystkie niezbędne biblioteki, ustawimy globalne parametry, załadujemy dataset CelebA i przygotujemy mechanizmy cache’owania embeddingów oraz generowania par obrazów.\n",
    "\n",
    "W tej sekcji zaimportujemy potrzebne biblioteki, pobierzemy zbiór par twarzy LFW, zdefiniujemy preprocessing, funkcje ekstrakcji embeddingów, przygotowania danych, model MLP, pętle treningu i ewaluacji z metrykami (accuracy, precision, recall, F1, ROC AUC).\n"
   ],
   "id": "1bf58e4910ddcab2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:01:59.906927Z",
     "start_time": "2025-06-24T18:01:33.483324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import fetch_lfw_pairs\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from PIL import ImageFilter, ImageEnhance\n",
    "\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 1.1. pobranie datasetu LFW (Labeled Faces in the Wild)\n",
    "lfw_train = fetch_lfw_pairs(subset='train', color=True, resize=0.5, download_if_missing=True)\n",
    "lfw_test  = fetch_lfw_pairs(subset='test',  color=True, resize=0.5, download_if_missing=True)\n",
    "\n",
    "# 1.2. preprocessing obrazów\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((160,160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "# 1.3. model facenet\n",
    "facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "def embed_from_array(arr):\n",
    "    img = transform(arr).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = facenet(img.unsqueeze(0))\n",
    "    return emb.squeeze(0)\n",
    "\n",
    "# 1.4. przygotowanie par obrazów\n",
    "def prepare_pairs(pairs, labels, sample_size=None, seed=None):\n",
    "    idxs = list(range(len(labels)))\n",
    "    if sample_size:\n",
    "        random.seed(seed)\n",
    "        idxs = random.sample(idxs, min(sample_size, len(idxs)))\n",
    "    X, y = [], []\n",
    "    for i in idxs:\n",
    "        a, b = pairs[i]\n",
    "        X.append(torch.abs(embed_from_array(a) - embed_from_array(b)))\n",
    "        y.append(labels[i])\n",
    "    X = torch.stack(X).to(device)\n",
    "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    return X, y\n",
    "\n",
    "# 1.5. definicja modelu MLP\n",
    "def make_mlp(input_dim=512):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, 256), nn.ReLU(),\n",
    "        nn.Linear(256, 64),  nn.ReLU(),\n",
    "        nn.Linear(64, 2)\n",
    "    ).to(device)\n",
    "\n",
    "# 1.6. trening i ewaluacja modelu\n",
    "def train_model(X, y, lr=1e-3, epochs=20):\n",
    "    model = make_mlp()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = crit(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return model\n",
    "\n",
    "def eval_model(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        probs = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    y_true = y.cpu().numpy()\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, preds),\n",
    "        'precision': precision_score(y_true, preds, zero_division=0),\n",
    "        'recall': recall_score(y_true, preds, zero_division=0),\n",
    "        'f1': f1_score(y_true, preds, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, probs)\n",
    "    }\n",
    "\n",
    "# przygotowanie wspólnego testu (200 par)\n",
    "test_X, test_y = prepare_pairs(lfw_test.pairs, lfw_test.target, sample_size=200, seed=42)\n",
    "print(\"Done\")\n"
   ],
   "id": "8b21c7b919904ee4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliks\\Documents\\GitHub\\zawodev\\PWR\\sem6\\AI\\zad5\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Done\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Zadanie 1: Wpływ rozmiaru zbioru treningowego\n",
    "\n",
    "Dla każdej wielkości `train_size`:\n",
    "- generujemy pary treningowe,\n",
    "- trenujemy MLP (20 epok, lr=1e-3),\n",
    "- ewaluujemy model na stałym zbiorze testowym (200 par)\n",
    "\n",
    "Metryki: accuracy, precision, recall, f1, roc_auc.\n"
   ],
   "id": "79b6d8fdb2c0ef1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:06:49.573326Z",
     "start_time": "2025-06-24T18:02:07.323559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_sizes = [10, 100, 500, 1000, 5000]\n",
    "results_size = {}\n",
    "\n",
    "for N in train_sizes:\n",
    "    X_tr, y_tr = prepare_pairs(\n",
    "        lfw_train.pairs, lfw_train.target,\n",
    "        sample_size=N, seed=1\n",
    "    )\n",
    "    mdl = train_model(X_tr, y_tr, lr=1e-3, epochs=20)\n",
    "    results_size[N] = eval_model(mdl, test_X, test_y)\n",
    "    print(f\"Train size={N}: {results_size[N]}\")\n"
   ],
   "id": "cbcc975377b74e3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size=10: {'accuracy': 0.795, 'precision': 0.8513513513513513, 'recall': 0.6774193548387096, 'f1': 0.7544910179640718, 'roc_auc': 0.9063410712491207}\n",
      "Train size=100: {'accuracy': 0.55, 'precision': 1.0, 'recall': 0.03225806451612903, 'f1': 0.0625, 'roc_auc': 0.9954778414229726}\n",
      "Train size=500: {'accuracy': 0.605, 'precision': 1.0, 'recall': 0.15053763440860216, 'f1': 0.2616822429906542, 'roc_auc': 0.9953773490101497}\n",
      "Train size=1000: {'accuracy': 0.635, 'precision': 1.0, 'recall': 0.21505376344086022, 'f1': 0.35398230088495575, 'roc_auc': 0.9954778414229726}\n",
      "Train size=5000: {'accuracy': 0.61, 'precision': 1.0, 'recall': 0.16129032258064516, 'f1': 0.2777777777777778, 'roc_auc': 0.9951763641845042}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Zadanie 2: Wpływ learning rate\n",
    "\n",
    "Dla stałej liczby par treningowych (1000) testujemy różne wartości learning rate:\n",
    "- lr=1e-4, 5e-4, 1e-3, 5e-3, 1e-2\n",
    "- trenujemy MLP (20 epok),\n",
    "- ewaluujemy model na stałym zbiorze testowym (200 par)\n",
    "\n",
    "Metryki: jak wyżej.\n"
   ],
   "id": "60f0111ca9d28ec4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:49:03.470972Z",
     "start_time": "2025-06-23T16:47:48.691159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lrs = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "X_1000, y_1000 = prepare_pairs(\n",
    "    lfw_train.pairs, lfw_train.target,\n",
    "    sample_size=1000, seed=2\n",
    ")\n",
    "results_lr = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    mdl = train_model(X_1000, y_1000, lr=lr, epochs=20)\n",
    "    results_lr[lr] = eval_model(mdl, test_X, test_y)\n",
    "    print(f\"LR={lr}: {results_lr[lr]}\")"
   ],
   "id": "4ab61c5fe985e108",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=0.0001: {'accuracy': 0.465, 'precision': 0.465, 'recall': 1.0, 'f1': 0.6348122866894198, 'roc_auc': 0.9925635614511105}\n",
      "LR=0.0005: {'accuracy': 0.54, 'precision': 1.0, 'recall': 0.010752688172043012, 'f1': 0.02127659574468085, 'roc_auc': 0.9956788262486183}\n",
      "LR=0.001: {'accuracy': 0.65, 'precision': 1.0, 'recall': 0.24731182795698925, 'f1': 0.39655172413793105, 'roc_auc': 0.9954778414229726}\n",
      "LR=0.005: {'accuracy': 0.93, 'precision': 0.9759036144578314, 'recall': 0.8709677419354839, 'f1': 0.9204545454545454, 'roc_auc': 0.9917596221485278}\n",
      "LR=0.01: {'accuracy': 0.91, 'precision': 1.0, 'recall': 0.8064516129032258, 'f1': 0.8928571428571429, 'roc_auc': 0.9953773490101497}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Zadanie 3: Wpływ liczby epok\n",
    "\n",
    "Dla stałej liczby par treningowych (1000) i optymalnego learning rate (np. 5e-3) testujemy różne liczby epok:\n",
    "- epochs=5, 10, 20, 50, 100\n",
    "- trenujemy MLP,\n",
    "- ewaluujemy model na stałym zbiorze testowym (200 par)\n",
    "\n",
    "Metryki: jak wyżej.\n"
   ],
   "id": "36c53f0ba88b111c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:54:10.770306Z",
     "start_time": "2025-06-23T16:54:09.594050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs_list = [5, 10, 20, 50, 100]\n",
    "results_epochs = {}\n",
    "\n",
    "for ep in epochs_list:\n",
    "    mdl = train_model(X_1000, y_1000, lr=1e-3, epochs=ep)\n",
    "    results_epochs[ep] = eval_model(mdl, test_X, test_y)\n",
    "    print(f\"Epochs={ep}: {results_epochs[ep]}\")"
   ],
   "id": "5f60188952d0bcc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs=5: {'accuracy': 0.56, 'precision': 1.0, 'recall': 0.053763440860215055, 'f1': 0.10204081632653061, 'roc_auc': 0.9948748869460355}\n",
      "Epochs=10: {'accuracy': 0.535, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'roc_auc': 0.9938699628178074}\n",
      "Epochs=20: {'accuracy': 0.64, 'precision': 1.0, 'recall': 0.22580645161290322, 'f1': 0.3684210526315789, 'roc_auc': 0.995176364184504}\n",
      "Epochs=50: {'accuracy': 0.94, 'precision': 0.9655172413793104, 'recall': 0.9032258064516129, 'f1': 0.9333333333333333, 'roc_auc': 0.9875389408099688}\n",
      "Epochs=100: {'accuracy': 0.915, 'precision': 0.8958333333333334, 'recall': 0.9247311827956989, 'f1': 0.91005291005291, 'roc_auc': 0.9800020098482565}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Zadanie 4: Scheduler i Early Stopping\n",
    "\n",
    "Chcemy dobrać najlepszy zestaw parametrów na podstawie poprzednich wyników, wprowadzić `ReduceLROnPlateau` i early stopping (patience=5) na val loss."
   ],
   "id": "1c96635a0a157ea8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:11:08.133039Z",
     "start_time": "2025-06-24T18:09:51.932209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_1000, y_1000 = prepare_pairs(\n",
    "    lfw_train.pairs, lfw_train.target,\n",
    "    sample_size=1000, seed=2\n",
    ")\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 4.1. Przygotowanie walidacji (80/20 split)\n",
    "dataset = TensorDataset(X_1000, y_1000)\n",
    "train_len = int(0.8 * len(dataset))\n",
    "val_len = len(dataset) - train_len\n",
    "train_ds, val_ds = torch.utils.data.random_split(\n",
    "    dataset, [train_len, val_len],\n",
    "    generator=torch.Generator().manual_seed(0)\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32)\n",
    "\n",
    "# 4.2. Model, optimizer, scheduler\n",
    "model = make_mlp()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=3\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_es = 5\n",
    "counter_es = 0\n",
    "\n",
    "# 4.3. Pętla tren–walidacja z early stopping\n",
    "for epoch in range(1, 101):\n",
    "    # trening\n",
    "    model.train()\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # walidacja\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            val_losses.append(criterion(model(x_batch), y_batch).item() * x_batch.size(0))\n",
    "    val_loss = sum(val_losses) / len(val_loader.dataset)\n",
    "\n",
    "    # scheduler i early stopping\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter_es = 0\n",
    "    else:\n",
    "        counter_es += 1\n",
    "        if counter_es >= patience_es:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} — val_loss: {val_loss:.4f} (best: {best_val_loss:.4f})\")\n"
   ],
   "id": "7a8d263713075dc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 — val_loss: 0.1978 (best: 0.1978)\n",
      "Epoch 002 — val_loss: 0.1685 (best: 0.1685)\n",
      "Epoch 003 — val_loss: 0.0797 (best: 0.0797)\n",
      "Epoch 004 — val_loss: 0.0897 (best: 0.0797)\n",
      "Epoch 005 — val_loss: 0.1050 (best: 0.0797)\n",
      "Epoch 006 — val_loss: 0.0908 (best: 0.0797)\n",
      "Epoch 007 — val_loss: 0.0910 (best: 0.0797)\n",
      "Early stopping at epoch 8\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Zadanie 5: Odporność na zaburzenia\n",
    "Chcemy przetestować model na zaburzonych obrazach i włączyć augmentacje do treningu, by podnieść odporność.\n",
    "\n",
    "Dodajemy do obu obrazów w parze:\n",
    "- szum Gaussa (σ=25)\n",
    "- rozmycie Gaussa (radius=3)\n",
    "- zwiększenie jasności (x1.5)\n",
    "\n",
    "Najpierw testujemy na zaburzonych testowych 200 parach, potem retrenujemy model (lr=5e-3, epoki=50) na danych z augmentacją."
   ],
   "id": "cd3f2af0ab3afd1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:15:52.537253Z",
     "start_time": "2025-06-24T18:11:15.136919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import ImageFilter, ImageEnhance\n",
    "\n",
    "def augment_array(arr, augment_type):\n",
    "    img = transforms.ToPILImage()(arr)\n",
    "    if augment_type == \"gauss\":\n",
    "        arr_np = np.array(img).astype(np.float32)\n",
    "        noise = np.random.normal(0,25,arr_np.shape)\n",
    "        img = Image.fromarray(np.clip(arr_np + noise, 0, 255).astype(np.uint8))\n",
    "    elif augment_type == \"blur\":\n",
    "        img = img.filter(ImageFilter.GaussianBlur(3))\n",
    "    elif augment_type == \"bright\":\n",
    "        img = ImageEnhance.Brightness(img).enhance(1.5)\n",
    "    return np.array(img)\n",
    "\n",
    "# 6.1. Test na zaburzonych\n",
    "metrics_aug = {}\n",
    "for aug_type in [\"gauss\",\"blur\",\"bright\"]:\n",
    "    X_aug, y_aug = [], []\n",
    "    for (a, b), lbl in zip(lfw_test.pairs[:200], lfw_test.target[:200]):\n",
    "        a2 = augment_array(a, aug_type)\n",
    "        b2 = augment_array(b, aug_type)\n",
    "        diff = torch.abs(embed_from_array(a2) - embed_from_array(b2))\n",
    "        X_aug.append(diff)\n",
    "        y_aug.append(lbl)\n",
    "    X_aug = torch.stack(X_aug).to(device)\n",
    "    y_aug = torch.tensor(y_aug, dtype=torch.long).to(device)\n",
    "    metrics_aug[aug_type] = eval_model(model, X_aug, y_aug)\n",
    "    print(f\"Aug={aug_type}: {metrics_aug[aug_type]}\")\n",
    "\n",
    "# 6.2. Retrain z augmentacją w train\n",
    "X_tr_list = [X_1000]\n",
    "y_tr_list = [y_1000.cpu()]\n",
    "for aug_type in [\"gauss\",\"blur\",\"bright\"]:\n",
    "    X_tmp, y_tmp = [], []\n",
    "    for (a, b), lbl in zip(lfw_train.pairs[:1000], lfw_train.target[:1000]):\n",
    "        a2 = augment_array(a, aug_type)\n",
    "        b2 = augment_array(b, aug_type)\n",
    "        emb_diff = torch.abs(embed_from_array(a2) - embed_from_array(b2)).cpu()\n",
    "        X_tmp.append(emb_diff)\n",
    "        y_tmp.append(lbl)\n",
    "    X_tr_list.append(torch.stack(X_tmp))\n",
    "    y_tr_list.append(torch.tensor(y_tmp, dtype=torch.long))\n",
    "\n",
    "X_tr_aug = torch.cat(X_tr_list, dim=0).to(device)\n",
    "y_tr_aug = torch.cat(y_tr_list, dim=0).to(device)\n",
    "\n",
    "model_aug = train_model(X_tr_aug, y_tr_aug, lr=5e-3, epochs=50)\n",
    "augmented_metrics = eval_model(model_aug, test_X, test_y)\n",
    "print(\"After training with augmentations:\", augmented_metrics)\n"
   ],
   "id": "bb08bb942cae1dd2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliks\\Documents\\GitHub\\zawodev\\PWR\\sem6\\AI\\zad5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug=gauss: {'accuracy': 0.68, 'precision': 1.0, 'recall': 0.68, 'f1': 0.8095238095238095, 'roc_auc': nan}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliks\\Documents\\GitHub\\zawodev\\PWR\\sem6\\AI\\zad5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug=blur: {'accuracy': 0.865, 'precision': 1.0, 'recall': 0.865, 'f1': 0.9276139410187667, 'roc_auc': nan}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliks\\Documents\\GitHub\\zawodev\\PWR\\sem6\\AI\\zad5\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug=bright: {'accuracy': 0.91, 'precision': 1.0, 'recall': 0.91, 'f1': 0.9528795811518325, 'roc_auc': nan}\n",
      "After training with augmentations: {'accuracy': 0.775, 'precision': 0.6739130434782609, 'recall': 1.0, 'f1': 0.8051948051948052, 'roc_auc': 0.9883428801125516}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Podsumowanie i wnioski\n",
    "- #### Zadanie 1 (rozmiar zbioru)\n",
    "  - N=10: dobra precyzja (0.85) ale niski recall (0.68) → model szybko overfituje pozytywy\n",
    "  - N=100–5000: precyzja = 1.0, bardzo niski recall (0.03–0.22) i stałe AUC około 0.995 -> MLP uczy się separacji, ale preferuje klasę „different”\n",
    "\n",
    "- #### Zadanie 2 (learning rate)\n",
    "  - lr=1e-4: recall = 1.0 ale wiele FP -> niska precyzja (0.68), AUC=0.98\n",
    "  - lr=5e-3–1e-2: najlepszy kompromis precision/recall około 0.87–0.98, accuracy około 0.91–0.93, AUC>0.99 -> optymalny zakres\n",
    "\n",
    "- #### Zadanie 3 (liczba epok)\n",
    "  - 5–10 epok: niewystarczająco nauki (recall bliski 0)\n",
    "  - 50–100 epok: high performance (accuracy około 0.94–0.915, F1 około 0.93–0.91), ale 100 epok drobny spadek AUC (overfitting)\n",
    "\n",
    "- #### Zadanie 4 (scheduler + early stopping)\n",
    "  - Val loss spadł do blisko 0.08 w 3 epokach, a early stopping zatrzymał trening na 8. epoce -> szybkość konwergencji i ochrona przed overfittingiem\n",
    "\n",
    "- #### Zadanie 5 (augmentacje)\n",
    "  - Test na zaburzonych: najlepsza odporność na rozmycie/bright (F1 około 0.93–0.95), na szum najgorsza (F1 około 0.81)\n",
    "  - Retraining z augmentacjami poprawił generalizację (accuracy -> 0.775, recall -> 1.0, F1 -> 0.81, AUC -> 0.99) w stosunku do bazowego\n",
    "\n",
    "Rekomendacje:\n",
    "- rozmiar zbioru: `1000` par daje najlepsze wyniki\n",
    "- tempo uczenia: około `5e-3` daje najlepszy kompromis precision/recall\n",
    "- liczba epok: `około 50` zapewnia wysokie f1 bez nadmiernego przetrenowania\n",
    "- scheduler i early stopping: reduceLROnPlateau z `patience 3` i early stopping z `patience 5` przyspieszają konwergencję i chronią przed overfittingiem\n",
    "- augmentacje: uwzględnienie zakłóconych przykładów typu `gauss`, `blur`, `bright` zwiększa odporność modelu"
   ],
   "id": "be4fa0d40e992608"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
